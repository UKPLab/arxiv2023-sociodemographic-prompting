{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.metrics.agreement import AnnotationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44840d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base sociodemograpics selected from toxicity_diverseperspectives dataset\n",
    "genders =  ['male', 'female']  # for now we ignore 'nonbinary'\n",
    "races = ['Black or African American', 'White', 'Asian',\n",
    "       'American Indian or Alaska Native', 'Hispanic',\n",
    "       'Native Hawaiian or Pacific Islander']\n",
    "educations = ['Some college but no degree',\n",
    "       'Associate degree in college (2-year)',\n",
    "       \"Bachelor's degree in college (4-year)\", 'Doctoral degree',\n",
    "       \"Master's degree\", 'Professional degree (JD, MD)',\n",
    "       'High school graduate (high school diploma or equivalent including GED)',\n",
    "       'Less than high school degree']\n",
    "age_ranges = ['35 - 44', '18 - 24', '25 - 34', '45 - 54', '55 - 64', '65 or older', 'Under 18']\n",
    "political_affiliations = ['Liberal', 'Independent', 'Conservative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_dispersion(array):\n",
    "    # https://www.statisticshowto.com/index-of-dispersion/\n",
    "    arr = np.array(array)\n",
    "    k = len(arr)\n",
    "    return np.divide( len(arr) * ( np.sum(arr) ** 2 - np.sum(arr**2)), (np.sum(arr) ** 2) * (len(arr)-1))\n",
    "\n",
    "def get_dispersion_indices(data, label_list):\n",
    "    dispersion_indices = []\n",
    "    for v in data.values():\n",
    "        p_counts = [0] * len(label_list)\n",
    "        for i in v['with_sd']:\n",
    "            p_counts[i] += 1\n",
    "        dispersion_indices.append(index_of_dispersion(p_counts))\n",
    "    return dispersion_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_old_samples(df, df_to_ignore, id_column):\n",
    "    old_sample_ids = list(df_to_ignore[id_column].values)\n",
    "    possibles = df[~df[id_column].isin(old_sample_ids)]\n",
    "    return possibles\n",
    "\n",
    "def sample_random(df, n, seed=1):\n",
    "    if len(df) < n:\n",
    "        raise ValueError(\"Number of possible samples is lower than provided n: \" + str(n))\n",
    "    samples = df.sample(n, random_state=seed)\n",
    "    return samples.to_dict(orient='records')     \n",
    "        \n",
    "def sample_dispersion(df, rating_col, label_col, label_list, n, min_dispersion, max_dispersion, seed=1):\n",
    "    samples = []\n",
    "    for i in df.to_dict(orient='records'):\n",
    "        counts_per_label = [0] * len(label_list)\n",
    "        for rating in i[rating_col]:\n",
    "            counts_per_label[label_list.index(rating[label_col])] += 1\n",
    "        dispersion_idx = index_of_dispersion(counts_per_label)\n",
    "        if min_dispersion < dispersion_idx < max_dispersion:\n",
    "            samples.append(i)\n",
    "    if len(samples) < n:\n",
    "        raise ValueError(\"Number of possible samples is lower than provided n: \" + str(n))\n",
    "    return pd.DataFrame(samples).sample(n, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20eb4d4",
   "metadata": {},
   "source": [
    "# Toxicity - Diverse Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e10ad1",
   "metadata": {},
   "source": [
    "## first we filter all ratings which did not answer any of the required socio-demographic attributes\n",
    "\n",
    "### Filters:\n",
    "- if one of the selected attributes was answered with \"Prefer not to say\"\n",
    "- if multiple 'race' attributes were selected\n",
    "- because of too few samples, we filtered the following attribute value (gender: 'Unknown', political_affilation: 'Other', race: 'Other', education: 'Other') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering methods based on dataset\n",
    "def filter_PreferNotToSay(row):\n",
    "    ratings_specified = []\n",
    "    for r in row.ratings:\n",
    "        try:\n",
    "            if any(['Prefer not to say' in r['gender'], 'Prefer not to say' in r['race'], \n",
    "                           'Prefer not to say' in r['education'], 'Prefer not to say' in r['age_range'],\n",
    "                           'Prefer not to say' in r['political_affilation']]):\n",
    "                ratings_specified.append(False)\n",
    "            else:\n",
    "                ratings_specified.append(True)\n",
    "        except:\n",
    "            ratings_specified.append(False)\n",
    "    return all(ratings_specified)\n",
    "\n",
    "def filter_multiRace(row):\n",
    "    ratings_specified = []\n",
    "    for r in row.ratings:\n",
    "        try:\n",
    "            if ',' in r['race']:\n",
    "                ratings_specified.append(False)\n",
    "            else:\n",
    "                ratings_specified.append(True)\n",
    "        except:\n",
    "            ratings_specified.append(False)\n",
    "    return all(ratings_specified)\n",
    "\n",
    "def filter_UnknownValues(row):\n",
    "    ratings_specified = []\n",
    "    for r in row.ratings:\n",
    "        try:\n",
    "            if 'Other' in r['race'] or 'Unknown' in r['gender'] or 'Other' in r['political_affilation'] or 'Other' in r['education']: \n",
    "                ratings_specified.append(False)\n",
    "            else:\n",
    "                ratings_specified.append(True)\n",
    "        except:\n",
    "            ratings_specified.append(False)\n",
    "    return all(ratings_specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/Users/tbeck/Repositories/context-stance/data/hatespeech/toxicity_ratings.json\"\n",
    "df = pd.read_json(p, lines=True, orient='records')\n",
    "print('Initial dataset size: {}'.format(len(df)))\n",
    "#df = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/hatespeech/toxicity_ratings_sample_with_sociodemographics_text-davinci-003.jsonl\", lines=True, orient='records')\n",
    "filtered_PreferNotToSay = df.apply(lambda row: filter_PreferNotToSay(row), axis=1)\n",
    "df = df[filtered_PreferNotToSay]\n",
    "print('After filtering empty attributes: {}'.format(len(df)))\n",
    "filtered_multiRace = df.apply(lambda row: filter_multiRace(row), axis=1)\n",
    "df = df[filtered_multiRace]\n",
    "print('After filtering multi-race attributes: {}'.format(len(df)))\n",
    "filtered_UnknownValues = df.apply(lambda row: filter_UnknownValues(row), axis=1)\n",
    "df = df[filtered_UnknownValues]\n",
    "print('After filtering less common attribute values: {}'.format(len(df)))\n",
    "data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a56048",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba28fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['comment_id'].unique()))\n",
    "label_list = [0,1,2,3,4]\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings']:\n",
    "        annot_task_data.append((idx, row['comment_id'], rating['toxic_score']))\n",
    "        counts_per_label[label_list.index(rating['toxic_score'])] += 1\n",
    "        if idx in worker_ids:\n",
    "            worker_ids[idx] += 1\n",
    "        else:\n",
    "            worker_ids[idx] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('Label Space: ', set([i[2] for i in annot_task_data]))\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e9195",
   "metadata": {},
   "source": [
    "## Next, we look at the sociodemographic attribute distribution in the remaining instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4943ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'toxic_score': [], 'gender': [], 'race': [], 'education': [], \n",
    "                  'age_range': [], 'political_affilation': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        for r in row.ratings:\n",
    "            attributes['toxic_score'].append(r['toxic_score'])\n",
    "            for attribute in ['gender', 'race', 'education', 'age_range', 'political_affilation']:\n",
    "                attributes[attribute].append(r[attribute])\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(df))\n",
    "atts['political_affilation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['race'].value_counts().plot(kind='bar', xlabel='Race', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['gender'].value_counts().plot(kind='bar', xlabel='Gender', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['education'].value_counts().plot(kind='bar', xlabel='Education', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['age_range'].value_counts().plot(kind='bar', xlabel='Age Range', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['political_affilation'].value_counts().plot(kind='bar', xlabel='Political Affilation', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#atts.groupby(['political_affilation']).agg('count')['toxic_score']\n",
    "for att in ['gender', 'race', 'education', 'age_range', 'political_affilation']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09502dab",
   "metadata": {},
   "source": [
    "## Sampling and checking sample's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2fe72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = df.sample(n=1000, random_state=seed)\n",
    "\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['gender', 'race', 'education', 'age_range', 'political_affilation']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()\n",
    "    \n",
    "#sample.to_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ff254",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_sample = remove_old_samples(df, sample, 'comment_id')\n",
    "#random_sample = sample_random(random_sample, 10, seed=1)\n",
    "dispersion_sample = remove_old_samples(df, sample, 'comment_id')\n",
    "label_list = [0,1,2,3,4]\n",
    "dispersion_sample = sample_dispersion(df, 'ratings', 'toxic_score', label_list, 10, 0.8, 1.0, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "toxic_fewshot_samples = []\n",
    "nontoxic_fewshot_samples = []\n",
    "for i in df.itertuples():\n",
    "    if i.comment_id in sample_comment_ids:\n",
    "        continue\n",
    "    \n",
    "    if maj_vote > 0:\n",
    "        toxic_fewshot_samples.append(i)\n",
    "    else:\n",
    "        nontoxic_fewshot_samples.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a473e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pd.DataFrame(nontoxic_fewshot_samples).sample(100).to_dict(orient='records'):\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f46528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(fewshot_samples).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12458d",
   "metadata": {},
   "source": [
    "# Sentiment Diaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ab57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read demographic information file\n",
    "sd= pd.read_csv(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/sentiment/diaz/dataverse_files_survey_demographic/demographics_responses.csv\")\n",
    "\n",
    "# if Hispanic/Latino==Yes, merge into main race column\n",
    "sd['Please indicate your race - Selected Choice'] = sd.apply(lambda x: 'Hispanic/Latino' if x['Are you Hispanic or Latino?']=='Yes' else x['Please indicate your race - Selected Choice'], axis=1)\n",
    "\n",
    "# rename columns \n",
    "column_map = {\n",
    "    'Please indicate your age': 'age_range',\n",
    "    'Please indicate your race - Selected Choice': 'race',\n",
    "    'Education': 'education',\n",
    "    'Please indicate your political identification': 'political_affiliation',\n",
    "    'Please indicate your gender': 'gender',\n",
    "    'How would you describe the area where you grew up?': 'area_origin',\n",
    "    'How would you describe the area in which you currently live?': 'area_living',\n",
    "    'In which region of the United States do you currently live?': 'us_region',\n",
    "    'Please indicate your annual household income': 'annual_household_income',\n",
    "    'Which employment status best describes you?': 'employment_status',\n",
    "    'How would you describe your current living situation (check all that apply) - Selected Choice': 'living_situation',\n",
    "}\n",
    "sd.rename(columns=column_map, inplace=True)\n",
    "# drop unused columns\n",
    "sd.drop(columns=['Please indicate your race - Other - Text', \n",
    "                 'Are you Hispanic or Latino?', \n",
    "                 'How would you describe your current living situation (check all that apply) - Other - Text'], \n",
    "        inplace=True)\n",
    "\n",
    "# merge 5-scale political affiliations into 3-scale \n",
    "political_affil_map = {\n",
    "    'Somewhat conservative': 'Conservative',\n",
    "    'Very conservative': 'Conservative',\n",
    "    'Moderate': 'Moderate',\n",
    "    'Somewhat liberal': 'Liberal',\n",
    "    'Very liberal': 'Liberal'\n",
    "}\n",
    "sd['political_affiliation'] = sd['political_affiliation'].apply(lambda x: political_affil_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data and merge with sociodemographic information\n",
    "df_train = pd.read_csv(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/sentiment/diaz/dataverse_files/train_older_adult_annotations.csv\", encoding='latin-1')\n",
    "df_train = pd.merge(df_train, sd, on='respondent_id', how='inner')\n",
    "df_train['split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ed174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for test data\n",
    "df_test = pd.read_csv(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/sentiment/diaz/dataverse_files/test_annotations.csv\", encoding='latin-1')\n",
    "df_test = pd.merge(df_test, sd, on='respondent_id', how='inner')\n",
    "df_test['split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4b3b4",
   "metadata": {},
   "source": [
    "## Filter data\n",
    "\n",
    "- filter \"Other\" from race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_UnknownValues(row):\n",
    "    ratings_specified = []\n",
    "    try:\n",
    "        if 'Other' in row['race']: \n",
    "            ratings_specified.append(False)\n",
    "        else:\n",
    "            ratings_specified.append(True)\n",
    "    except:\n",
    "        ratings_specified.append(False)\n",
    "    return all(ratings_specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc64525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial dataset size: {}'.format(len(df)))\n",
    "filtered_UnknownValues = df.apply(lambda row: filter_UnknownValues(row), axis=1)\n",
    "df = df[filtered_UnknownValues]\n",
    "print('After filtering \"Other\" from race: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25288a93",
   "metadata": {},
   "source": [
    "## Bring data into format similar to toxicity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate ratings per text instance into one dataset row\n",
    "df_agg = []\n",
    "for unit_id in df['unit_id'].unique():\n",
    "    # get all annotations about same unit\n",
    "    n = {\n",
    "        'unit_id': unit_id,\n",
    "        'ratings': []\n",
    "    }\n",
    "    for row in df[df['unit_id']==unit_id].to_dict(orient='records'):\n",
    "        if 'unit_text' not in n:\n",
    "            n['unit_text'] = row['unit_text']\n",
    "        rating = {**row}\n",
    "        del rating['unit_id']  # already contained in n\n",
    "        del rating['unit_text']  # already contained in n\n",
    "        n['ratings'].append(rating)\n",
    "    df_agg.append(n)\n",
    "    \n",
    "df_agg = pd.DataFrame(df_agg)\n",
    "data = df_agg\n",
    "#df_agg.to_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/sentiment/diaz/data.jsonl\", lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7d1c9",
   "metadata": {},
   "source": [
    "## Analysis of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7728d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['unit_id'].unique()))\n",
    "label_list = ['Very positive', 'Somewhat positive', 'Neutral', 'Somewhat negative', 'Very negative']\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings']:\n",
    "        annot_task_data.append((rating['respondent_id'], row['unit_id'], rating['annotation']))\n",
    "        counts_per_label[label_list.index(rating['annotation'])] += 1\n",
    "        if rating['respondent_id'] in worker_ids:\n",
    "            worker_ids[rating['respondent_id']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['respondent_id']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('Label Space: ', set([i[2] for i in annot_task_data]))\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc76868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'sentiment': [], 'gender': [], 'race': [], 'education': [], \n",
    "                  'age_range': [], 'political_affiliation': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        for r in row.ratings:\n",
    "            attributes['sentiment'].append(r['annotation'])\n",
    "            for attribute in ['gender', 'race', 'education', 'age_range', 'political_affiliation']:\n",
    "                attributes[attribute].append(r[attribute])\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(df_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfddea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['race'].value_counts().plot(kind='bar', xlabel='Race', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cf77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['gender'].value_counts().plot(kind='bar', xlabel='Gender', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ad05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['education'].value_counts().plot(kind='bar', xlabel='Education', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['age_range'].value_counts().plot(kind='bar', xlabel='Age Range', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33008a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['political_affiliation'].value_counts().plot(kind='bar', xlabel='Political Affiliation', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102abde5",
   "metadata": {},
   "source": [
    "## Sampling and checking sample's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c05955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in ['gender', 'race', 'education', 'age_range', 'political_affiliation']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5849ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = df_agg.sample(n=1000, random_state=seed)\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['gender', 'race', 'education', 'age_range', 'political_affiliation']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()\n",
    "\n",
    "sample.to_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/sentiment/diaz/sentiment_diaz_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab0be9",
   "metadata": {},
   "source": [
    "# Stance - SemEval2016T6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca946796",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'favor',\n",
    "    1: 'against',\n",
    "    2: 'none',\n",
    "    'against': 'against',\n",
    "    'favor': 'favor',\n",
    "    'none': 'none',\n",
    "    'neutral': 'none'\n",
    "}\n",
    "df = pd.read_csv(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/stance/semeval2016t6/SemEval2016-Task6-raw-annotations-stance.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52825e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read gold label data\n",
    "import csv\n",
    "bp = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/stance/semeval2016t6/data-all-annotations/\"\n",
    "def read(train_file, trial_file, test_file, split_file):\n",
    "    X, y = [], []\n",
    "\n",
    "    with open(train_file, \"r\", encoding=\"ISO-8859-1\") as in_train_f,\\\n",
    "    open(trial_file, \"r\", encoding=\"ISO-8859-1\") as in_trial_f,\\\n",
    "    open(test_file, \"r\", encoding=\"ISO-8859-1\") as in_test_f,\\\n",
    "    open(split_file, \"r\") as in_split_file:\n",
    "\n",
    "        data_train = list(csv.reader(in_train_f, delimiter='\\t', quotechar = '\"', ))\n",
    "        data_trial = list(csv.reader(in_trial_f, delimiter='\\t', quotechar = '\"', ))\n",
    "        data_test = list(csv.reader(in_test_f, delimiter='\\t', quotechar = '\"', ))\n",
    "\n",
    "        for row in in_split_file.readlines():\n",
    "            data_file, line_number = row.rstrip().split(\"_\")\n",
    "            line_number = int(line_number)\n",
    "\n",
    "            if data_file == \"train\":\n",
    "                X.append((data_train[line_number][1], data_train[line_number][2]))\n",
    "                y.append(data_train[line_number][3])\n",
    "            elif data_file == \"trial\":\n",
    "                X.append((data_trial[line_number][1], data_trial[line_number][2]))\n",
    "                y.append(data_trial[line_number][3])\n",
    "            else:\n",
    "                X.append((data_test[line_number][1], data_test[line_number][2]))\n",
    "                y.append(data_test[line_number][3])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# read and tokenize data\n",
    "X_train, y_train = read(os.path.join(bp, \"trainingdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"trialdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"testdata-taskA-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"semeval2016t6_train.csv\"))\n",
    "X_dev, y_dev = read(os.path.join(bp, \"trainingdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"trialdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"testdata-taskA-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"semeval2016t6_dev.csv\"))\n",
    "X_test, y_test = read(os.path.join(bp, \"trainingdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"trialdata-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"testdata-taskA-all-annotations.txt\"),\n",
    "                        os.path.join(bp, \"semeval2016t6_test.csv\"))\n",
    "\n",
    "assert len(X_train) == 2497\n",
    "assert len(X_dev) == 417\n",
    "assert len(X_test) == 1249\n",
    "\n",
    "data = []\n",
    "for x, y, split in [(X_train, y_train, 'train'), (X_dev, y_dev, 'dev'), (X_test, y_test, 'test')]:\n",
    "    for (target, text), label in zip(x,y):\n",
    "        data.append({\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "            'target': target,\n",
    "            'split': split\n",
    "        })\n",
    "gold_data = pd.DataFrame(data, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e617427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into common format (but without sociodemographic info -> ratings_without_sd)\n",
    "data = {}\n",
    "for i in df.to_dict(orient='records'):\n",
    "    item = {'worker_id': i['Worker ID'], 'stance': label_map[i['Stance'].lower()], \n",
    "            'opinion_towards': i['Opinion towards']}\n",
    "    if i['Tweet'] in data:\n",
    "        data[i['Tweet']]['ratings_without_sd'].append(item)\n",
    "    else:\n",
    "        data[i['Tweet']] = {\n",
    "            'text': i['Tweet'],\n",
    "            'target': i['Target'],\n",
    "            'instance_id': i['Instance ID'],\n",
    "            'ratings_without_sd': [item]\n",
    "        }\n",
    "# merge gold label into common format\n",
    "for i in gold_data.to_dict(orient='records'):\n",
    "    if i['text'] in data:\n",
    "        data[i['text']]['hard_gold'] = label_map[i['label'].lower()]\n",
    "        data[i['text']]['split'] = i['split']\n",
    "\n",
    "# Filter all instances which are not contained in the gold data files (they also dont have any instance-id associated)\n",
    "data = pd.DataFrame(data.values()).dropna()\n",
    "# remove dataset-specific hashtag from texts\n",
    "data['text'] = data['text'].apply(lambda x: x.replace('#SemST', '').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e83bc5",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77482c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['instance_id'].unique()))\n",
    "print('Label Space: ', data['hard_gold'].unique())\n",
    "label_list = ['favor','against','none']\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for row in data.to_dict(orient='records'):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings_without_sd']:\n",
    "        annot_task_data.append((rating['worker_id'], row['instance_id'], rating['stance']))\n",
    "        counts_per_label[label_list.index(rating['stance'])] += 1\n",
    "        if rating['worker_id'] in worker_ids:\n",
    "            worker_ids[rating['worker_id']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['worker_id']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'stance': [], 'target': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        attributes['stance'].append(row.hard_gold)\n",
    "        attributes['target'].append(row.target)\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7250ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['stance'].value_counts().plot(kind='bar', xlabel='Stance', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['target'].value_counts().plot(kind='bar', xlabel='Target', ylabel='Count', rot=90, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e701a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in ['stance', 'target']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70c53",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae89791",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = data.sample(n=1000, random_state=seed)\n",
    "# read sociodemographic information from toxicity dataset and add sociodemographic information\n",
    "toxicity_sd_samples = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')\n",
    "sample['ratings'] = toxicity_sd_samples['ratings'].tolist()\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['stance', 'target']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b68127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data\n",
    "sample.to_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/stance/semeval2016t6/stance_semeval2016t6_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8cc31",
   "metadata": {},
   "source": [
    "# Stance - GWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b24274",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/stance/gwsd/\"\n",
    "label_dict = {\n",
    "    'agrees': 'agree',\n",
    "    'disagrees': 'disagree',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "def get_stance(item):\n",
    "    #  we simply choose the majority from the three labels as done in the original paper\n",
    "    argmax_label = np.argmax([item['disagree'], item['agree'], item['neutral']])\n",
    "    majority_label = ['disagree', 'agree', 'neutral'][argmax_label]\n",
    "    return majority_label\n",
    "\n",
    "def get_ratings_without_sd(item):\n",
    "    ratings_withoud_sd = []\n",
    "    for worker_idx in range(8):\n",
    "        ratings_withoud_sd.append({'worker_idx': worker_idx, 'annotation': label_dict[item['worker_' + str(worker_idx)]]})\n",
    "    return ratings_withoud_sd\n",
    "\n",
    "data = []\n",
    "df = pd.read_csv(os.path.join(bp, \"GWSD.tsv\"), sep='\\t')\n",
    "for i in df.itertuples(index=False):\n",
    "    row = i._asdict()\n",
    "    if np.all(np.isnan([row['disagree'], row['agree'], row['neutral']])):\n",
    "        # these instances have not been considered for the annotated dataset in the original paper\n",
    "        continue\n",
    "    data.append({\n",
    "        'text': row['sentence'],\n",
    "        'hard_gold': get_stance(row),\n",
    "        'target': 'Climate change/global warming is a serious concern', # see paper for details (https://arxiv.org/pdf/2010.15149.pdf\n",
    "        'split': 'test' if row['in_held_out_test'] else 'train',\n",
    "        'ratings_without_sd': get_ratings_without_sd(row),\n",
    "    })\n",
    "\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset='text', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608c1d2",
   "metadata": {},
   "source": [
    "##  Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['text'].unique()))\n",
    "print('Label Space: ', data['hard_gold'].unique())\n",
    "label_list = ['agree','disagree','neutral']\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings_without_sd']:\n",
    "        annot_task_data.append((rating['worker_idx'], idx, rating['annotation']))\n",
    "        counts_per_label[label_list.index(rating['annotation'])] += 1\n",
    "        if rating['worker_idx'] in worker_ids:\n",
    "            worker_ids[rating['worker_idx']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['worker_idx']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc15b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = data.sample(n=1000, random_state=seed)\n",
    "# read sociodemographic information from toxicity dataset and add sociodemographic information\n",
    "toxicity_sd_samples = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')\n",
    "sample['ratings'] = toxicity_sd_samples['ratings'].tolist()\n",
    "sample.to_json(os.path.join(bp, 'stance_gwsd_filtered_sample_n1000_seed{}.jsonl'.format(seed)), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087af895",
   "metadata": {},
   "source": [
    "# Toxicity - Jigsaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to the Kaggle competition website:\n",
    "# The competition target was a binarized version of the toxicity column, \n",
    "# which can be easily reconstructed using a >=0.5 threshold.\n",
    "bp = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/toxicity/jigsaw/\"\n",
    "all_data = pd.read_csv(os.path.join(bp, \"all_data.csv\")) # merged data but without individual votes\n",
    "individual_annotations = pd.read_csv(os.path.join(bp, \"toxicity_individual_annotations.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in all_data.to_dict(orient='records'):\n",
    "    data[i['id']] = {**i}\n",
    "    data[i['id']]['hard_gold'] = 1 if i['toxicity'] >= 0.5 else 0\n",
    "for i in individual_annotations.to_dict(orient='records'):\n",
    "    if 'ratings_without_sd' in data[i['id']]:\n",
    "        data[i['id']]['ratings_without_sd'].append({**i})\n",
    "    else:\n",
    "        data[i['id']]['ratings_without_sd'] = [{**i}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523119b",
   "metadata": {},
   "source": [
    "### Filter instances where we dont have original annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size: ', len(data))\n",
    "data.dropna(subset='ratings_without_sd', inplace=True)\n",
    "print('After filter entries without original annotations:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3818e4d",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['id'].unique()))\n",
    "print('Label Space: ', data['hard_gold'].unique())\n",
    "label_list = [0,1]\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings_without_sd']:\n",
    "        annot_task_data.append((rating['worker'], row['id'], rating['toxic']))\n",
    "        counts_per_label[label_list.index(rating['toxic'])] += 1\n",
    "        if rating['worker'] in worker_ids:\n",
    "            worker_ids[rating['worker']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['worker']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddaea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'hatespeech': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        attributes['hatespeech'].append(row.hard_gold)\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(data))\n",
    "for att in ['hatespeech']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = data.sample(n=1000, random_state=seed)\n",
    "\n",
    "# read sociodemographic information from toxicity dataset and add sociodemographic information\n",
    "toxicity_sd_samples = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')\n",
    "sample['ratings'] = toxicity_sd_samples['ratings'].tolist()\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['hatespeech']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()\n",
    "sample.to_json(os.path.join(bp, 'toxicity_jigsaw_filtered_sample_n1000_seed{}.jsonl'.format(seed)), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23f940",
   "metadata": {},
   "source": [
    "# Hate Speech - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0732ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/hatespeech/twitter/\"\n",
    "annotations = pd.read_csv(os.path.join(bp, \"amateur_expert_annotations.csv\"), sep=\"\\t\", dtype=str) # tweet IDs + annotations\n",
    "full_data = pd.read_json(os.path.join(bp, \"amateur_expert.json\"), lines=True, orient='records', dtype=str) # twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc225781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings(row):\n",
    "    ratings = [{'annotator_id': 'expert', 'annotation': row['Expert']}]\n",
    "    for i in range(1090):\n",
    "        k = 'Amateur_' + str(i)\n",
    "        if k in row and not pd.isna(row[k]):\n",
    "            ratings.append({'annotator_id': k.lower(), 'annotation': row[k]})\n",
    "    return ratings\n",
    "\n",
    "data = {}\n",
    "for i in full_data.to_dict(orient='records'):\n",
    "    data[i['id']] = {**i}\n",
    "    data[i['id']]['hard_gold'] = data[i['id']].pop('Annotation').lower()\n",
    "for i in annotations.to_dict(orient='records'):\n",
    "    data[i['TweetID']]['ratings_without_sd'] = get_ratings(i)\n",
    "\n",
    "data = pd.DataFrame(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the gold label is the one chosen by the experts\n",
    "for i in data.to_dict(orient='records'):\n",
    "    expert = [x['annotation'] for x in i['ratings_without_sd'] if x['annotator_id']=='expert'][0]\n",
    "    assert i['hard_gold'] == expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008bef5",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed403c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['id'].unique()))\n",
    "print('Label Space: ', data['hard_gold'].unique())\n",
    "label_list = ['sexism', 'neither', 'racism', 'both']\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings_without_sd']:\n",
    "        annot_task_data.append((rating['annotator_id'], row['id'], rating['annotation']))\n",
    "        counts_per_label[label_list.index(rating['annotation'])] += 1\n",
    "        if rating['annotator_id'] in worker_ids:\n",
    "            worker_ids[rating['annotator_id']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['annotator_id']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'hatespeech': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        attributes['hatespeech'].append(row.hard_gold)\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(data))\n",
    "for att in ['hatespeech']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2bf94",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30096222",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = data.sample(n=1000, random_state=seed)\n",
    "\n",
    "# read sociodemographic information from toxicity dataset and add sociodemographic information\n",
    "toxicity_sd_samples = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')\n",
    "sample['ratings'] = toxicity_sd_samples['ratings'].tolist()\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['hatespeech']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()\n",
    "#sample.to_json(os.path.join(bp, 'hatespeech_twitter_filtered_sample_n1000_seed{}.jsonl'.format(seed)), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440db20b",
   "metadata": {},
   "source": [
    "# Hate Speech - GHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30269bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/hatespeech/ghc/\"\n",
    "df = pd.read_csv(os.path.join(bp, \"GabHateCorpus_annotations.tsv\"), sep=\"\\t\", dtype=str)\n",
    "df.fillna('0', inplace=True)\n",
    "train = pd.read_csv(os.path.join(bp, \"ghc_train.tsv\"), sep=\"\\t\")\n",
    "test = pd.read_csv(os.path.join(bp, \"ghc_test.tsv\"), sep=\"\\t\")\n",
    "annotator_info = pd.read_csv(os.path.join(bp, \"AnnotatorIAT_and_Attitudes.csv\"))\n",
    "\n",
    "annot_info = {}\n",
    "for i in annotator_info.to_dict(orient='records'):\n",
    "    annot_info[i['Annotator']] = i\n",
    "    del annot_info[i['Annotator']]['Annotator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hatespeech(row):\n",
    "    if any([row['hd'], row['cv'], row['vo']]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "data = {}\n",
    "for i in df.to_dict(orient='records'):\n",
    "    cop = i.copy()\n",
    "    del cop['Text']\n",
    "    del cop['ID']\n",
    "    try:\n",
    "        cop = {**cop, **annot_info[cop['Annotator']]}\n",
    "    except:\n",
    "        cop = {**cop}\n",
    "    if i['Text'] in data:\n",
    "        data[i['Text']]['ratings_without_sd'].append(cop)\n",
    "    else:\n",
    "        data[i['Text']] = {\n",
    "            'text': i['Text'],\n",
    "            'unit_id': i['ID'],\n",
    "            'ratings_without_sd': [cop]\n",
    "        }\n",
    "\n",
    "# training data\n",
    "for i in train.to_dict(orient='records'):\n",
    "    data[i['text']]['hard_gold'] = 1 if is_hatespeech(i) else 0\n",
    "    data[i['text']]['split'] = 'train'\n",
    "# test data\n",
    "for i in test.to_dict(orient='records'):\n",
    "    data[i['text']]['hard_gold'] = 1 if is_hatespeech(i) else 0\n",
    "    data[i['text']]['split'] = 'test'\n",
    "    \n",
    "data = pd.DataFrame(data.values())\n",
    "print('Before Filtering: ', len(data))\n",
    "data.dropna(subset='hard_gold', inplace=True)\n",
    "print('After Filtering: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['hard_gold'].isna()]#data['ratings_without_sd'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9425e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37087cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATASET')\n",
    "print('Number of all entries:', len(data))\n",
    "print('Unique text instances:', len(data['unit_id'].unique()))\n",
    "print('Label Space: ', data['hard_gold'].unique())\n",
    "label_list = ['0', '1']\n",
    "worker_ids = {}\n",
    "dispersion_indices = []\n",
    "annot_task_data = []\n",
    "for idx, row in enumerate(data.to_dict(orient='records')):\n",
    "    counts_per_label = [0] * len(label_list)\n",
    "    for rating in row['ratings_without_sd']:\n",
    "        annot_task_data.append((rating['Annotator'], row['unit_id'], rating['Hate']))\n",
    "        counts_per_label[label_list.index(rating['Hate'])] += 1\n",
    "        if rating['Annotator'] in worker_ids:\n",
    "            worker_ids[rating['Annotator']] += 1\n",
    "        else:\n",
    "            worker_ids[rating['Annotator']] = 1\n",
    "    dispersion_indices.append(index_of_dispersion(counts_per_label))\n",
    "at = AnnotationTask(data=annot_task_data)\n",
    "print('ANNOTATIONS')\n",
    "print('Unique annotators:', len(worker_ids.keys()))\n",
    "print('avg nr of annotations per instance (stdev): {:.2f} ({:.2f})'.format(np.mean([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')]), np.std([len(i['ratings_without_sd']) for i in data.to_dict(orient='records')])))\n",
    "print('avg nr of annotations per annotator (stdev): {:.2f} ({:.2f})'.format(np.mean(list(worker_ids.values())), np.std(list(worker_ids.values()))))\n",
    "print('Krippendorf alpha: {:.2f}'.format(at.alpha()))\n",
    "print('Dispersion Index (stdev): {:.2f} ({:.2f})'.format(np.mean(dispersion_indices), np.std(dispersion_indices)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fc786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_attributes(df):\n",
    "    attributes = {'hatespeech': []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        attributes['hatespeech'].append(row.hard_gold)\n",
    "    return attributes\n",
    "\n",
    "atts = pd.DataFrame(get_attributes(data))\n",
    "for att in ['hatespeech']:\n",
    "    print(atts[att].value_counts() / len(atts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e052fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "sample = data.sample(n=1000, random_state=seed)\n",
    "\n",
    "# read sociodemographic information from toxicity dataset and add sociodemographic information\n",
    "toxicity_sd_samples = pd.read_json(\"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results/toxicity/diverse_perspectives/toxicity_diverse_perspectives_filtered_sample_n1000_seed{}.jsonl\".format(seed), lines=True, orient='records')\n",
    "sample['ratings'] = toxicity_sd_samples['ratings'].tolist()\n",
    "sample_atts = pd.DataFrame(get_attributes(sample))\n",
    "for att in ['hatespeech']:\n",
    "    print(sample_atts[att].value_counts() / len(sample_atts))\n",
    "    print()\n",
    "sample.to_json(os.path.join(bp, 'hatespeech_ghc_filtered_sample_n1000_seed{}.jsonl'.format(seed)), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4bae8",
   "metadata": {},
   "source": [
    "## Few-Shot Sampling\n",
    "\n",
    "- read original data file and file with prompting responses\n",
    "- sample n instances based on selection strategy (dispersion, random)\n",
    "- if no gold label, use majority vote label (name it \"hard_gold\"); else use gold label (\"hard_gold\")\n",
    "- write to output file\n",
    "\n",
    "One few-shot sample should contain:\n",
    "- text\n",
    "- gold label (hard_gold)\n",
    "- original annotations (in case of sd-datasets use the ratings, in case of non-sd-datasets use the ratings_without_sd)\n",
    "- prompted annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fa766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filter_isalnum_but_not_space(x):\n",
    "    return str.isalnum(x) or str.isspace(x)\n",
    "\n",
    "def decide_goldlabel(g):\n",
    "    if type(g) == str:\n",
    "        return g.lower()\n",
    "    else:\n",
    "        return g\n",
    "    \n",
    "def get_majority_vote(ratings, label_column):\n",
    "    maj_vote = Counter([i[label_column] for i in ratings]).most_common(1)[0][0] # majority vote\n",
    "    if type(maj_vote) == str:\n",
    "        return maj_vote.strip().lower()\n",
    "    return maj_vote\n",
    "\n",
    "def get_dispersion_index(ratings, label_list, label_column):\n",
    "    p_counts = [0] * len(label_list)\n",
    "    for rating in ratings:\n",
    "        if type(rating[label_column]) == str and rating[label_column].strip().lower() in label_list:\n",
    "            p_counts[label_list.index(rating[label_column].strip().lower())] += 1\n",
    "        elif type(rating[label_column]) == int:\n",
    "            p_counts[rating[label_column]] += 1\n",
    "        else:\n",
    "            raise ValueError(\"Wrong label\", rating[label_column])\n",
    "    return index_of_dispersion(p_counts)\n",
    "\n",
    "    \n",
    "def get_label(task, response, label_list, label_conversion):\n",
    "    if task in ['toxicity_jigsaw', 'hatespeech_ghc']:\n",
    "        for answer in ['yes', 'no']:\n",
    "            if response.startswith(answer):\n",
    "                return (True, answer)\n",
    "        return (False, response)\n",
    "    if task in ['toxicity_diverse_perspectives', 'sentiment_diaz', 'stance_semeval2016t6', 'stance_gwsd','hatespeech_twitter']:\n",
    "        for answer in label_list:\n",
    "            if response.startswith(answer):\n",
    "                return (True, answer)\n",
    "            elif response in label_conversion:\n",
    "                return (True, label_list[label_conversion[response]])\n",
    "        return (False, response)\n",
    "    raise ValueError('Unknown dataset: ', task)\n",
    "    \n",
    "label_maps = {'toxicity_diverse_perspectives': ['not toxic', 'slightly toxic', 'moderately toxic', 'very toxic', 'extremely toxic'],\n",
    "             'sentiment_diaz': ['very negative', 'somewhat negative', 'neutral', 'somewhat positive', 'very positive'],\n",
    "             'toxicity_jigsaw': ['yes', 'no'],\n",
    "              'stance_semeval2016t6': ['favor', 'against', 'none'],\n",
    "              'stance_gwsd': ['agree', 'disagree', 'neutral'],\n",
    "              'hatespeech_twitter': ['neither', 'sexism', 'both', 'racism'],\n",
    "              'hatespeech_ghc': ['yes', 'no']\n",
    "             }\n",
    "\n",
    "original_labels = {'toxicity_diverse_perspectives': ['not toxic', 'slightly toxic', 'moderately toxic', 'very toxic', 'extremely toxic'],\n",
    "             'sentiment_diaz': ['very negative', 'somewhat negative', 'neutral', 'somewhat positive', 'very positive'],\n",
    "             'toxicity_jigsaw': ['0', '1'],\n",
    "              'stance_semeval2016t6': ['favor', 'against', 'none'],\n",
    "              'stance_gwsd': ['agree', 'disagree', 'neutral'],\n",
    "              'hatespeech_twitter': ['neither', 'sexism', 'both', 'racism'],\n",
    "              'hatespeech_ghc': ['0', '1']\n",
    "}\n",
    "\n",
    "label_conversions = {'toxicity_diverse_perspectives': {**{k:idx for idx,k in enumerate(label_maps['toxicity_diverse_perspectives'])}, **{idx:idx for idx in range(len(label_maps['toxicity_diverse_perspectives']))}, 'toxicity': 2},\n",
    "                'sentiment_diaz': {**{k:idx for idx,k in enumerate(label_maps['sentiment_diaz'])},  **{idx:idx for idx in range(len(label_maps['sentiment_diaz']))},  'somewhat nostalgic': label_maps['sentiment_diaz'].index('neutral')},\n",
    "                'toxicity_jigsaw': {'yes': 1,'no': 0,1: 1,0: 0,'1': 1,'0': 0, 'toxic': 1,'no this text does not contain any language that is considered toxic': 0, 'yes this text contains language that is derogatory and dismissive of transgender people which is considered a form of hate speech': 1, 'yes this text contains language that is derogatory and dismissive of transgender people which is considered a form of hate speech it also implies that mannings actions led to the deaths of people which could be seen as an attempt to incite violence': 1, 'no this text does not contain any language that is considered toxic it is discussing a political issue in a factual and respectful manner':0, 'nothe us has saved millions of lives in africa through the implementation of health initiatives and aid programs to increase access to medical care and combat infectious and preventable diseases such as malaria hivaids and tuberculosis these initiatives have also assisted in improving access to safe drinking water sanitation and nutrition which have been essential for reducing child mortality rates': 0, 'no \\n\\ngiven the context of the': 0},\n",
    "                 'stance_semeval2016t6': {**{k:idx for idx,k in enumerate(label_maps['stance_semeval2016t6'])},  **{idx:idx for idx in range(len(label_maps['stance_semeval2016t6']))}},\n",
    "                 'stance_gwsd': {**{k:idx for idx,k in enumerate(label_maps['stance_gwsd'])},  **{idx:idx for idx in range(len(label_maps['stance_gwsd']))}, 'stance': 2},\n",
    "                 'hatespeech_twitter': {**{k:idx for idx,k in enumerate(label_maps['hatespeech_twitter'])},  **{idx:idx for idx in range(len(label_maps['hatespeech_twitter']))}, 'both sexism and racism': label_maps['hatespeech_twitter'].index('both'), 'both racism and sexism': label_maps['hatespeech_twitter'].index('both')},\n",
    "                 'hatespeech_ghc': {**{k:idx for idx,k in enumerate(label_maps['hatespeech_ghc'])},  **{idx:idx for idx in range(len(label_maps['hatespeech_ghc']))}, 'yes': 1, 'no': 0, 'hatespeech': 1, 'no the text does not contain any hate speech it is simply praising alex jones and gavin mcinnes and promoting the infowars official app': 0}\n",
    "                }\n",
    "\n",
    "column_names = {'toxicity_diverse_perspectives': ('comment_id', 'comment', 'toxic_score', 'ratings'),\n",
    "               'sentiment_diaz': ('unit_id', 'unit_text', 'annotation', 'ratings'),\n",
    "               'stance_semeval2016t6': ('instance_id', 'text', 'stance', 'ratings_without_sd'),\n",
    "               'stance_gwsd': ('text', 'text', 'annotation', 'ratings_without_sd'),\n",
    "               'toxicity_jigsaw': ('id',  'comment_text', 'toxic', 'ratings_without_sd'),\n",
    "               'hatespeech_ghc': ('unit_id', 'text', 'Hate', 'ratings_without_sd'),\n",
    "               'hatespeech_twitter': ('id', 'text', 'annotation', 'ratings_without_sd')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/tbeck/Repositories/context-stance/data/prompting_sociodemographic/server_results\"\n",
    "task = \"toxicity\"\n",
    "dataset = \"jigsaw\"\n",
    "task_dataset = task + \"_\" + dataset\n",
    "seed = 24\n",
    "n = 1000\n",
    "attribute = \"ratings\"\n",
    "model = \"text-davinci-003\"\n",
    "format_id = 0\n",
    "orig_fn = os.path.join(base, task, dataset, \"_\".join([task, dataset, \"filtered\", \"sample\", \"n\"+str(n), \"seed\"+str(seed)]) + \".jsonl\")\n",
    "prompted_fn = os.path.join(base, task, dataset, \"_\".join([task, dataset, \"filtered\", \"sample\", \"n\"+str(n), \"seed\"+str(seed), \"with_sociodemographics\", attribute, model, \"format\"+str(format_id)]) + \".jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(prompted_fn, orient='records', lines=True, convert_dates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643169a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_conversion = label_conversions[task_dataset]\n",
    "label_list = label_maps[task_dataset]\n",
    "id_column, text_column, label_column, ratings_column = column_names[task_dataset]\n",
    "data = {}\n",
    "for row in df.to_dict(orient='records'):\n",
    "    ratings = row['ratings']\n",
    "    response = \"\".join(filter(filter_isalnum_but_not_space, row['response'])).strip().lower()\n",
    "    (validResponse, answer) = get_label(task_dataset, response, label_list, label_conversion)\n",
    "    assert answer in label_list  # assert correct label conversion\n",
    "    response = answer\n",
    "    \n",
    "    if row[id_column] in data:\n",
    "        data[row[id_column]]['prompt_ratings'].append({'response': response, \n",
    "                                                       'prompt': row['prompt'],\n",
    "                                                       **ratings[row['rating_idx']]})\n",
    "    else:\n",
    "        columns_to_ignore = list(row['ratings'][0].keys())\n",
    "        columns_to_ignore += ['rating_idx', 'age', 'political_affiliation', 'prompt', 'response']\n",
    "        columns_to_consider = [k for k in row.keys() if k not in columns_to_ignore]\n",
    "        data[row[id_column]] = {k: row[k] for k in columns_to_consider}\n",
    "        data[row[id_column]]['prompt_ratings'] = [{'response': response, \n",
    "                                                   'prompt': row['prompt'],\n",
    "                                                   **ratings[row['rating_idx']]}]\n",
    "        \n",
    "converted_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "# remove old rating profiles from toxicity dataset for non-sd-datasets\n",
    "if task_dataset not in ['toxicity_diverse_perspectives', 'sentiment_diaz']:\n",
    "    converted_df.drop(labels='ratings', axis='columns', inplace=True)\n",
    "\n",
    "# get majority vote as gold label\n",
    "if task_dataset in ['toxicity_diverse_perspectives', 'sentiment_diaz']:\n",
    "    converted_df['ratings_majority'] = converted_df['ratings'].apply(lambda x: get_majority_vote(x, label_column))\n",
    "else:\n",
    "    converted_df['ratings_majority'] = converted_df['ratings_without_sd'].apply(lambda x: get_majority_vote(x, label_column))\n",
    "converted_df['prompt_ratings_majority'] = converted_df['prompt_ratings'].apply(lambda x: get_majority_vote(x, 'response'))\n",
    "\n",
    "# get dispersion indices\n",
    "orig_label_list = original_labels[task_dataset]\n",
    "if task_dataset in ['toxicity_diverse_perspectives', 'sentiment_diaz']:\n",
    "    converted_df['ratings_dispersion'] = converted_df['ratings'].apply(lambda x: get_dispersion_index(x, orig_label_list, label_column))\n",
    "else:\n",
    "    converted_df['ratings_dispersion'] = converted_df['ratings_without_sd'].apply(lambda x: get_dispersion_index(x, orig_label_list, label_column))\n",
    "converted_df['prompt_ratings_dispersion'] = converted_df['prompt_ratings'].apply(lambda x: get_dispersion_index(x, label_list, 'response'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = os.path.join(base, task, dataset, task_dataset + \"_fewshot_samples_n{n}_seed{seed}_{model}.json\".format(n=n, seed=seed, model=model))\n",
    "sample_output = {}\n",
    "sample_seeds = [10,11,12]\n",
    "for sample_seed in sample_seeds:\n",
    "    sample_output[sample_seed] = {}\n",
    "    for k in [5,10,15]:\n",
    "        # random sampling\n",
    "        random_sample = converted_df.sample(k, random_state=sample_seed).to_dict(orient='records')\n",
    "        assert len(converted_df[converted_df['prompt_ratings_dispersion'] > 0.5]) > k\n",
    "        assert len(converted_df[converted_df['ratings_dispersion'] > 0.5]) > k\n",
    "        high_prompt_dispersion_sample = converted_df[converted_df['prompt_ratings_dispersion'] > 0.5].sample(k, random_state=sample_seed).to_dict(orient='records')\n",
    "        high_ratings_dispersion_sample = converted_df[converted_df['ratings_dispersion'] > 0.5].sample(k, random_state=sample_seed).to_dict(orient='records')\n",
    "        sample_output[sample_seed][k] = {\n",
    "            'random': random_sample,\n",
    "            'high_prompt_dispersion': high_prompt_dispersion_sample,\n",
    "            'high_ratings_dispersion': high_ratings_dispersion_sample\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f23eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contextstance",
   "language": "python",
   "name": "contextstance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
